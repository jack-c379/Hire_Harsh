import os
import yaml
from pathlib import Path
from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional, Dict, Any
from pydantic import BaseModel


class LLMConfig(BaseModel):
    """Configuration for the Large Language Model (LLM) settings."""

    provider: str = "openai"
    model: str = "gpt-4o-mini"
    temperature: float = 0.1
    timeout: int = 60


class OllamaConfig(BaseModel):
    """Configuration for the Ollama model endpoint and parameters."""

    endpoint: str = "http://localhost:11434"
    models: str = "llama3.2"
    timeout: int = 120


class EmbeddingConfig(BaseModel):
    """Configuration for embedding model parameters."""

    provider: str = "openai"
    model: str = "text-embedding-3-small"
    chunk_size: int = 1000
    chunk_overlap: int = 200


class VectorStoreConfig(BaseModel):
    """Configuration for vector store settings."""

    provider: str = "chroma"
    persist_directory: str = "data/vector_db"
    collection_name: str = "hire_harsh"
    retrieval_k: int = 8


class RateLimitConfig(BaseModel):
    """Configuration for rate limiting API requests."""

    enabled: bool = True
    requests_per_minute: int = 20
    burst_limit: int = 5
    rate_limit_ms: int = 3000


class SecurityConfig(BaseModel):
    """Configuration for security-related settings."""

    session_timeout_hours: int = 24
    max_query_length: int = 1000
    max_job_text_length: int = 5000
    min_job_text_length: int = 50
    allowed_origins: list[str] = ["*"]
    secure_cookies: bool = True


class LoggingConfig(BaseModel):
    """Configuration for logging behavior and file management."""

    level: str = "INFO"
    file_path: str = "logs/hire_harsh.log"
    max_file_size_mb: int = 100
    backup_count: int = 5
    analytics_enabled: bool = True


class DataPaths(BaseModel):
    """Configuration for various data file paths used by the application."""

    cv_path: str = "data/Harsh Jaiswal Resume 4Dec.pdf"
    about_me_path: str = "data/about_me.md"
    vector_db_path: str = "data/vector_db"
    analytics_log_path: str = "data/analytics.log"


class CandidateConfig(BaseModel):
    """Configuration for candidate information displayed in the application."""

    name: str = "Harsh Jaiswal"
    email: str = "2harshjaiswal@gmail.com"
    linkedin: str = "https://www.linkedin.com/in/harshjaiswal/"
    github: str = "https://github.com/jack-c379/"


class AppConfig(BaseSettings):
    """
    Main application configuration class that aggregates all sub-configurations.

    This includes environment settings, API keys, model configurations,
    rate limiting, security, logging, data paths, and invite codes.
    """

    environment: str = Field(default="development")
    debug: bool = False
    openai_api_key: Optional[str] = Field(default=None, alias="OPENAI_API_KEY")
    langsmith_api_key: Optional[str] = Field(default=None, alias="LANGSMITH_API_KEY")

    llm: LLMConfig = LLMConfig()
    ollama: OllamaConfig = OllamaConfig()
    embedding: EmbeddingConfig = EmbeddingConfig()
    vectorstore: VectorStoreConfig = VectorStoreConfig()
    rate_limit: RateLimitConfig = RateLimitConfig()
    security: SecurityConfig = SecurityConfig()
    logging: LoggingConfig = LoggingConfig()
    data: DataPaths = DataPaths()
    candidate: CandidateConfig = CandidateConfig()
    invite_codes: Dict[str, Any] = {}

    # Environment variables for invite codes
    invite_codes_env: Optional[str] = Field(default=None, alias="INVITE_CODES")

    # Chat fallback response for guardrails
    chat_fallback_response: str = (
        "I'm sorry, but I cannot provide a response to that query."
    )

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        extra = "allow"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._load_invite_codes_from_env()

    def _load_invite_codes_from_env(self):
        """Load invite codes from environment variables."""
        import json

        # If INVITE_CODES environment variable is set, use it
        if self.invite_codes_env:
            try:
                env_codes = json.loads(self.invite_codes_env)
                self.invite_codes.update(env_codes)
            except json.JSONDecodeError as e:
                print(
                    f"Warning: Invalid JSON in INVITE_CODES environment variable: {e}"
                )


def deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recursively merge two dictionaries.

    Values from the override dictionary take precedence over those in the base dictionary.
    Nested dictionaries are merged recursively.

    Args:
        base (Dict[str, Any]): The base dictionary to be merged into.
        override (Dict[str, Any]): The dictionary with overriding values.

    Returns:
        Dict[str, Any]: The merged dictionary.
    """
    for k, v in override.items():
        if isinstance(v, dict) and isinstance(base.get(k), dict):
            base[k] = deep_merge(base[k], v)
        else:
            base[k] = v
    return base


def load_config() -> AppConfig:
    """
    Load and merge configuration from YAML files and environment variables.

    Loads the base configuration from 'config/base.yml' and environment-specific overrides
    from 'config/{ENVIRONMENT}.yml'. Then overlays these settings with environment variables
    and returns a fully constructed AppConfig instance.

    Returns:
        AppConfig: The loaded and merged application configuration.
    """
    env = os.getenv("ENVIRONMENT", "development")
    base = Path("config/base.yml")
    override = Path(f"config/{env}.yml")

    merged_data: dict[str, Any] = {}

    if base.exists():
        with open(base, "r") as f:
            merged_data = yaml.safe_load(f) or {}

    if override.exists():
        with open(override, "r") as f:
            override_data = yaml.safe_load(f) or {}
            merged_data = deep_merge(merged_data, override_data)

    # Load .env + YAML merged settings
    return AppConfig(**merged_data)


config = load_config()
print(config.llm.provider)
